{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24f1a13-989f-4a51-b3c2-8fcbb3e34b31",
   "metadata": {},
   "source": [
    "## Use For Processing Textual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9154afb-72d9-48fd-bc06-84349375ac49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\manshay\\anaconda3\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\manshay\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\manshay\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\manshay\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\manshay\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\manshay\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\manshay\\anaconda3\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b8222d-2b75-4506-921f-e3905b620d3c",
   "metadata": {},
   "source": [
    "## Import Libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439e6e65-aeee-45b3-b001-1c47abf4d8b1",
   "metadata": {},
   "source": [
    "#### Interacting With System and to make files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bede408c-29e8-42d5-bb42-a94b37765115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea56973c-6521-4e2e-ac56-d44c864c5044",
   "metadata": {},
   "source": [
    "#### Use for data manipulation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2492fed-43aa-4a38-bb93-0f81ebfb2447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbc337-39e3-4e74-827f-b5fef8a42b85",
   "metadata": {},
   "source": [
    "#### regular expression support for searh string and patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3f55f93-f5b7-41e8-9640-7c6770dd2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5712abc7-fdf0-4544-9f64-3200ad125ff2",
   "metadata": {},
   "source": [
    "#### for linguistic analysis as well as corpus analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad4cec8e-d482-4361-87cd-e21357bd3448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cmudict, stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495636f8-543a-4935-a297-f17654b9a274",
   "metadata": {},
   "source": [
    "#### Using for split text into words or sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc1a99ab-3a12-4061-8e53-07de22ec19ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66e0065-a063-4f53-8208-53f340684e10",
   "metadata": {},
   "source": [
    "#### Processing textual data check the word for checking one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75ee5c79-2ffd-42b6-8b9a-7deb56cb64d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c439fb7-cc3f-4102-8b12-12f7b4885625",
   "metadata": {},
   "outputs": [],
   "source": [
    "Use for storing data ex list, tuples, arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcd1821b-0fe1-4794-812b-8117e516d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fee3130-f966-4d7b-a4f4-fb6dbf65048e",
   "metadata": {},
   "source": [
    "#### Use for NLP for text data analysis and triggered so many things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c19cfb30-d391-4192-b8fd-7ef13e51ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d5d54c-fd90-4f41-9e24-65c4b908fa45",
   "metadata": {},
   "source": [
    "#### Module in nltk to learn parameters from a corpus. I am use that to change pargraph to convert into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9253ea67-da64-4cb8-9ab1-b75d12db6296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Manshay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba41ce44-b41f-4cf7-89cb-8dac8ef304d8",
   "metadata": {},
   "source": [
    "#### used for tagging words with their parts of speech "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d969b09f-dbb3-4c92-9381-ac44c806d658",
   "metadata": {},
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac00d9b0-efc2-4e90-b535-f823f92dddb8",
   "metadata": {},
   "source": [
    "#### using for Filteration Elimitate stopwords ex \n",
    "##### i am driving the car (stop words)\n",
    "##### driving car (without stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b87c76ce-6a7d-430e-a8bf-5c901194caad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Manshay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210da00b-7a15-4a89-80cc-fd7619196852",
   "metadata": {},
   "source": [
    "#### use to expose the data with little or no assumption on how it is to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5deee068-7b0e-4d71-ae44-3684e88a7aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\Manshay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('cmudict')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229cc04e-5a11-45f4-b7e9-6bd96e05b80f",
   "metadata": {},
   "source": [
    "#### it use for checking the word is pronouncable or not that word will exits or not that's why we use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6cb2d0f-8b82-4a41-bcea-60226ea0a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = cmudict.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fb8c74-4560-4270-b61f-f2c83522d6d1",
   "metadata": {},
   "source": [
    "#### Count Syllables : Count the number of word in the website \n",
    "#### first we convert all in lower case and looking in dictornay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "486bafd1-e7c2-4773-bbe8-5e17f35bdb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):\n",
    "    if word.lower() in d:\n",
    "        return [len(list(y for y in x if y[-1].isdigit())) for x in d [word.lower()]][0]\n",
    "    else:\n",
    "        return 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3df7f4-b344-48a4-bb02-d5e56a3611cf",
   "metadata": {},
   "source": [
    "#### Readability Metrics : Apply formula giving in text analysis  calculates various readability metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c54ae69-08ed-42c4-b1f4-3a5f78fb074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_metrics(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    complex_words = [word for word in words if count_syllables(word) >= 3]\n",
    "    word_count = len(words)\n",
    "    sentence_count = len(sentences)\n",
    "    avg_sentence_length = word_count / sentence_count\n",
    "    complex_word_count = len(complex_words)\n",
    "    percentage_complex_words = complex_word_count / word_count * 100\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    avg_words_per_sentence = word_count / sentence_count\n",
    "    syllable_count = sum([count_syllables(word) for word in words])\n",
    "    avg_syllables_per_word = syllable_count / word_count\n",
    "    avg_word_length = sum(len(word) for word in words) / word_count\n",
    "    \n",
    "    return{\n",
    "        'avg_sentence_length' : avg_sentence_length,\n",
    "        'percentage_complex_words' : percentage_complex_words,\n",
    "        'fog_index' : fog_index,\n",
    "        'avg_words_per_sentence' : avg_words_per_sentence,\n",
    "        'complex_word_count': complex_word_count,\n",
    "        'word_count' : word_count,\n",
    "        'avg_syllables_per_word' : avg_syllables_per_word,\n",
    "        'avg_word_length' : avg_word_length\n",
    "    }      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b310b6-2619-4d21-8cb6-73339a499912",
   "metadata": {},
   "source": [
    "#### Sentiment Scores : checking the words from the given positive and negative word file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76e4366e-befc-4b6a-be78-c7e1cd26050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores(text):\n",
    "    blob = TextBlob(text)\n",
    "    positive_score = sum(1 for word in blob.words if word.lower() in positive_words)\n",
    "    negative_score = sum(1 for word in blob.words if word.lower() in negative_words)\n",
    "    polarity_score = blob.sentiment.polarity\n",
    "    subjectivity_score = blob.sentiment.subjectivity\n",
    "    \n",
    "    return{\n",
    "        'positive_score' : positive_score,\n",
    "        'negative_score' : negative_score,\n",
    "        'polarity_score' : polarity_score,\n",
    "        'subjectivity_score' : subjectivity_score\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0f8d34-3b4f-4a99-ae46-cdaecb24ef57",
   "metadata": {},
   "source": [
    "#### Load Positive and Negative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8020df6-f942-45a4-a17d-d79b79666e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('positive-words.txt', 'r') as f:\n",
    "    positive_words = set(f.read().split())\n",
    "\n",
    "with open('negative-words.txt', 'r') as f:\n",
    "    negative_words = set(f.read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a19de5c-6e30-4d0f-a8a7-f88a58a61fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'extracted_articles'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f564300f-5a75-41c1-b69b-f88f2efd58c1",
   "metadata": {},
   "source": [
    "##### Empty List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f815797-06de-468f-87e2-1aae5887be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9654a80-b718-4533-9bb1-7d1ef76079dd",
   "metadata": {},
   "source": [
    "#### 1-2 iterate the overall file\n",
    "#### 3-4 contruct the file path and watch one by one URL ID\n",
    "#### 5-6 Read file\n",
    "#### 7 extract \n",
    "#### 8 calculate metric readanility\n",
    "#### 9 calculate Sentiment scores\n",
    "#### 10 Count Personal Pronouns\n",
    "#### append result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1d26224-a08b-482f-9c52-d9c57b1dc07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        url_id = os.path.splitext(filename)[0]\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        title, article_text = text.split('\\n',1)\n",
    "        readability = readability_metrics(article_text)\n",
    "        sentiment = sentiment_scores(article_text)\n",
    "        personal_pronouns = len([word for word, pos in nltk.pos_tag(word_tokenize(article_text)) if pos == 'PRP'])\n",
    "        result.append({\n",
    "            'URL_ID' : url_id,\n",
    "            'POSITIVE SCORE' : sentiment['positive_score'],\n",
    "            'NEGATIVE SCORE' : sentiment['negative_score'],\n",
    "            'POLARITY SCORE' : sentiment['polarity_score'],\n",
    "            'SUBJECTIVITY SCORE': sentiment['subjectivity_score'],\n",
    "            'AVG SENTENCE LENGTH': readability['avg_sentence_length'],\n",
    "            'PERCENTAGE OF COMPLEX WORDS': readability['percentage_complex_words'],\n",
    "            'FOG INDEX': readability['fog_index'],\n",
    "            'AVG NUMBER OF WORDS PER SENTENCE': readability['avg_words_per_sentence'],\n",
    "            'COMPLEX WORD COUNT': readability['complex_word_count'],\n",
    "            'WORD COUNT': readability['word_count'],\n",
    "            'SYLLABLES PER WORD': readability['avg_syllables_per_word'],\n",
    "            'PERSONAL PRONOUNS': personal_pronouns,\n",
    "            'AVG WORD LENGTH': readability['avg_word_length']\n",
    "        })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6325678-a239-42a4-b70c-58cf74dcadd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610de182-1c46-4afb-8ff0-3a5ec8aeb9f1",
   "metadata": {},
   "source": [
    "##### Save result in Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3f2fc4b-8f68-4edf-8cbe-f20d851d0f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to output_data_structure.xlsx\n"
     ]
    }
   ],
   "source": [
    "output_path = 'output_data_structure.xlsx'\n",
    "result_df.to_excel(output_path, index=False)\n",
    "print(f'Results saved to {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f65469-a66e-4581-b0d8-10f67ddcfd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
